import cv2
import face_recognition
import pickle
import sys
import os
from huggingface_hub import hf_hub_download
from ultralytics import YOLO
from insightface.app import FaceAnalysis
import cv2, numpy as np, pathlib, pickle, os
import matplotlib.pyplot as plt

# ===== 1. åŸºæœ¬åƒæ•¸ =====
VIDEO_IN   = ""
VIDEO_OUT  = ""
DB_PKL     = "face_db.pkl"
CONF       = 0.30
TOLERANCE  = 0.4  # é¤˜å¼¦ç›¸ä¼¼åº¦è¶Šæ¥è¿‘1è¶Šç›¸ä¼¼
MARGIN     = 0.4
# ç›´æ¥å¾ Hugging Face å–æ¬Šé‡
weight = hf_hub_download("arnabdhar/YOLOv8-Face-Detection", "model.pt")

# ===== 2. è¼‰å…¥æ¨¡å‹ & è³‡æ–™åº« =====
print("â–¶  è¼‰å…¥ YOLOv8-Face èˆ‡ InsightFace â€¦")
face_yolo = YOLO(weight)
face_app = FaceAnalysis(name="buffalo_l", providers=["CPUExecutionProvider"])
face_app.prepare(ctx_id=0)

db_vecs, db_names = None, None

# ===== 4. å–å¾—å‘é‡ï¼ˆArcFaceï¼‰ =====
def get_embedding(img_bgr, bbox, margin=MARGIN):
    x1,y1,x2,y2 = bbox
    h, w = img_bgr.shape[:2]
    dx, dy = int((x2-x1)*margin), int((y2-y1)*margin)
    l = max(x1-dx, 0); t = max(y1-dy, 0)
    r = min(x2+dx, w); b = min(y2+dy, h)
    face_crop = img_bgr[t:b, l:r]
    face_rgb = cv2.cvtColor(face_crop, cv2.COLOR_BGR2RGB)
    faces = face_app.get(face_rgb)
    if faces:
        emb = faces[0].embedding
        emb = emb / np.linalg.norm(emb)
        return emb
    return None


if __name__ == "__main__":
    VIDEO_IN = sys.argv[1]
    VIDEO_OUT = sys.argv[2]
    if len(sys.argv) != 3:
        print("ç”¨æ³•ï¼špython recognizer.py è¼¸å…¥å½±ç‰‡.mp4 è¼¸å‡ºå½±ç‰‡.mp4")
    else:

        if os.path.exists(DB_PKL):
            db = pickle.load(open(DB_PKL, "rb"))
            db_vecs  = np.stack([d["vec"] for d in db])
            db_names = [d["name"] for d in db]
            print(f"â–¶  å·²è¼‰å…¥äººè‡‰è³‡æ–™åº«ï¼š{len(db_names)} å¼µå‘é‡")
        else:
            print("âš ï¸  æ‰¾ä¸åˆ° face_db.pklï¼Œå°‡åªæ¨™ç¤º Unknown")

        # ===== 3. é–‹å•Ÿå½±ç‰‡ =====
        cap = cv2.VideoCapture(VIDEO_IN)
        if not cap.isOpened():
            raise FileNotFoundError(f"ç„¡æ³•é–‹å•Ÿå½±ç‰‡ï¼š{VIDEO_IN}")

        fps   = cap.get(cv2.CAP_PROP_FPS) or 30
        w     = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        h     = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        fourc = cv2.VideoWriter_fourcc(*"mp4v")
        out   = cv2.VideoWriter(VIDEO_OUT, fourc, fps, (w, h))


        # ===== 5. ä¸»è¿´åœˆ =====
        print("ğŸš€  é–‹å§‹è¾¨è­˜ â€¦ (ESC é›¢é–‹)")
        plt.ion()  # é–‹å•Ÿå³æ™‚äº’å‹•æ¨¡å¼
        fig, ax = plt.subplots()
        img_plot = None

        while True:
            ok, frame = cap.read()
            if not ok:
                break

            # 5-1 YOLO åµæ¸¬
            res = face_yolo(frame, conf=CONF, verbose=False)[0]

            # 5-2 å°æ¯å€‹ bbox åšç·¨ç¢¼ & æ¯”å°
            for box in res.boxes.xyxy.cpu().numpy():
                x1,y1,x2,y2 = map(int, box)
                emb = get_embedding(frame, (x1, y1, x2, y2))
                if emb is None:
                    continue

                # æ¯”å°è³‡æ–™åº«ï¼ˆé¤˜å¼¦ç›¸ä¼¼åº¦ï¼‰
                name, score_txt = "Unknown", ""
                if db_vecs is not None:
                    sims = db_vecs @ emb  # é¤˜å¼¦ç›¸ä¼¼åº¦
                    idx = np.argmax(sims)
                    if sims[idx] > TOLERANCE:
                        name = db_names[idx]
                    score_txt = f"{sims[idx]:.2f}"

                # ç•«æ¡† + æ¨™ç±¤
                cv2.rectangle(frame, (x1,y1), (x2,y2), (0,255,0), 2)
                cv2.putText(frame, f"{name} {score_txt}",
                            (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX,
                            0.6, (0,255,0), 2, cv2.LINE_AA)

            # 5-3 å¯«å…¥ç•«é¢
            out.write(frame)

            # 5-4 ä½¿ç”¨ matplotlib é¡¯ç¤ºç•«é¢
            cv2.imshow("YOLO-Face-Match", frame)
            if cv2.waitKey(1) & 0xFF == 27:  # æŒ‰ ESC é›¢é–‹
                break

        # ===== 6. æ”¶å°¾ =====
        cap.release(); out.release(); plt.ioff(); plt.close()
        print(f"âœ…  è¼¸å‡ºå®Œæˆ â†’ {pathlib.Path(VIDEO_OUT).resolve()}")